{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88cd38a",
   "metadata": {},
   "source": [
    "# BYOC training for paddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2365a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG REGISTRY_URI\n",
      "FROM ${REGISTRY_URI}/pytorch-training:1.6.0-gpu-py36-cu101-ubuntu16.04\n",
      "\n",
      "RUN mkdir -p /opt/ml/model\n",
      "\n",
      "ENV PYTHONUNBUFFERED=TRUE\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      "ENV PATH=\"/opt/program:${PATH}\"\n",
      "\n",
      "##########################################################################################\n",
      "# SageMaker requirements\n",
      "##########################################################################################\n",
      "## install flask\n",
      "RUN pip install networkx==2.3 flask gevent gunicorn boto3 transformers==4.6.0 datasets==1.11.0 sentencepiece==0.1.91 pytorch_lightning==0.8.1 jieba editdistance -i https://opentuna.cn/pypi/web/simple\n",
      "### Install nginx notebook\n",
      "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
      "         wget \\\n",
      "         nginx \\\n",
      "         ca-certificates \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# forward request and error logs to docker log collector\n",
      "RUN ln -sf /dev/stdout /var/log/nginx/access.log\n",
      "RUN ln -sf /dev/stderr /var/log/nginx/error.log\n",
      "\n",
      "# Set up the program in the image\n",
      "COPY * /opt/program/\n",
      "#use the model file from s3 directly\n",
      "#COPY model/* /opt/program/model/\n",
      "WORKDIR /opt/prgram\n",
      "RUN chmod +x /opt/program/train\n",
      "RUN chmod +x /opt/program/serve\n",
      "#ENV SAGEMAKER_PROGRAM /opt/ml/code/train.py\n",
      "#ENTRYPOINT [\"python\", \"train.py\"]\n",
      "#ENTRYPOINT [\"python\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "598bdb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set -e\n",
      "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
      "# by SageMaker.\n",
      "\n",
      "# The argument to this script is the image name. This will be used as the image on the local\n",
      "# machine and combined with the account and region to form the repository name for ECR.\n",
      "image=$1\n",
      "\n",
      "if [ \"$image\" == \"\" ]\n",
      "then\n",
      "    echo \"Use image name absa\"\n",
      "    image=\"absa\"\n",
      "fi\n",
      "\n",
      "# Get the account number associated with the current IAM credentials\n",
      "account=$(aws sts get-caller-identity --query Account --output text)\n",
      "\n",
      "if [ $? -ne 0 ]\n",
      "then\n",
      "    exit 255\n",
      "fi\n",
      "\n",
      "# Get the region defined in the current configuration\n",
      "region=$(aws configure get region)\n",
      "#regions=$(aws ec2 describe-regions --all-regions --query \"Regions[].{Name:RegionName}\" --output text)\n",
      "\n",
      "#for region in $regions; do\n",
      "\n",
      "#aws s3 cp s3://aws-solutions-${region}/spot-bot-models/cars/model.tar.gz ./\n",
      "#tar zxvf model.tar.gz\n",
      "# TODO: update regional location based on https://amazonaws-china.com/releasenotes/available-deep-learning-containers-images/\n",
      "\n",
      "if [[ $region =~ ^cn.* ]]\n",
      "then\n",
      "    fullname=\"${account}.dkr.ecr.${region}.amazonaws.com.cn/${image}:latest\"\n",
      "    registry_id=\"727897471807\"\n",
      "    registry_uri=\"${registry_id}.dkr.ecr.${region}.amazonaws.com.cn\"\n",
      "elif [[ $region = \"ap-east-1\" ]]\n",
      "then\n",
      "    fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:latest\"\n",
      "    registry_id=\"871362719292\"\n",
      "    registry_uri=\"${registry_id}.dkr.ecr.${region}.amazonaws.com\"\n",
      "else\n",
      "    fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:latest\"\n",
      "    registry_id=\"763104351884\"\n",
      "    registry_uri=\"${registry_id}.dkr.ecr.${region}.amazonaws.com\"\n",
      "fi\n",
      "\n",
      "echo ${fullname}\n",
      "847380964353.dkr.ecr.us-west-2.amazonaws.com/absa-train:latest\n",
      "\n",
      "# If the repository doesn't exist in ECR, create it.\n",
      "aws ecr describe-repositories --repository-names \"${image}\" --region ${region} || aws ecr create-repository --repository-name \"${image}\" --region ${region}\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-west-2:847380964353:repository/absa-train\",\n",
      "            \"registryId\": \"847380964353\",\n",
      "            \"repositoryName\": \"absa-train\",\n",
      "            \"repositoryUri\": \"847380964353.dkr.ecr.us-west-2.amazonaws.com/absa-train\",\n",
      "            \"createdAt\": 1652413724.0,\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            },\n",
      "            \"encryptionConfiguration\": {\n",
      "                \"encryptionType\": \"AES256\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "# Get the login command from ECR and execute it directly\n",
      "$(aws ecr get-login --registry-ids ${account} --region ${region} --no-include-email)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "$(aws ecr get-login --registry-ids ${registry_id} --region ${region} --no-include-email)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "# Build the docker image, tag with full name and then push it to ECR\n",
      "docker build -t ${image} -f Dockerfile . --build-arg REGISTRY_URI=${registry_uri}\n",
      "Sending build context to Docker daemon  410.1kB\n",
      "Step 1/15 : ARG REGISTRY_URI\n",
      "Step 2/15 : FROM ${REGISTRY_URI}/pytorch-training:1.6.0-gpu-py36-cu101-ubuntu16.04\n",
      " ---> 30e42e4701a4\n",
      "Step 3/15 : RUN mkdir -p /opt/ml/model\n",
      " ---> Using cache\n",
      " ---> 1808669d6e84\n",
      "Step 4/15 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> c1518f9004ed\n",
      "Step 5/15 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> cf17b08599ab\n",
      "Step 6/15 : ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 7dab4e69bac1\n",
      "Step 7/15 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 9ecd1c42e1c3\n",
      "Step 8/15 : RUN pip install networkx==2.3 flask gevent gunicorn boto3 transformers==4.6.0 datasets==1.11.0 sentencepiece==0.1.91 pytorch_lightning==0.8.1 jieba editdistance -i https://opentuna.cn/pypi/web/simple\n",
      " ---> Using cache\n",
      " ---> cecdac784abb\n",
      "Step 9/15 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 697d297ad851\n",
      "Step 10/15 : RUN ln -sf /dev/stdout /var/log/nginx/access.log\n",
      " ---> Using cache\n",
      " ---> 2726530008d8\n",
      "Step 11/15 : RUN ln -sf /dev/stderr /var/log/nginx/error.log\n",
      " ---> Using cache\n",
      " ---> ddd2796b12e2\n",
      "Step 12/15 : COPY * /opt/program/\n",
      " ---> aba0f8371b72\n",
      "Step 13/15 : WORKDIR /opt/prgram\n",
      " ---> Running in 0eb6467c934b\n",
      "Removing intermediate container 0eb6467c934b\n",
      " ---> 6498e4759522\n",
      "Step 14/15 : RUN chmod +x /opt/program/train\n",
      " ---> Running in 7b44a6ddf5bd\n",
      "Removing intermediate container 7b44a6ddf5bd\n",
      " ---> c91fb2c8e511\n",
      "Step 15/15 : RUN chmod +x /opt/program/serve\n",
      " ---> Running in 2a02bfde79b2\n",
      "Removing intermediate container 2a02bfde79b2\n",
      " ---> fec3b12d0cfa\n",
      "Successfully built fec3b12d0cfa\n",
      "Successfully tagged absa-train:latest\n",
      "docker tag ${image} ${fullname}\n",
      "docker push ${fullname}\n",
      "The push refers to repository [847380964353.dkr.ecr.us-west-2.amazonaws.com/absa-train]\n",
      "\n",
      "\u001b[1B39306cf2: Preparing \n",
      "\u001b[1Bc5668509: Preparing \n",
      "\u001b[1Ba70e188d: Preparing \n",
      "\u001b[1Baa9f3cdb: Preparing \n",
      "\u001b[1B1cf67dbd: Preparing \n",
      "\u001b[1Bd4cc46ae: Preparing \n",
      "\u001b[1Bc5d8d9a8: Preparing \n",
      "\u001b[1Bc5995fc5: Preparing \n",
      "\u001b[1Bb6526b31: Preparing \n",
      "\u001b[1Bf885ee05: Preparing \n",
      "\u001b[1Be2eca2aa: Preparing \n",
      "\u001b[1B047bee3b: Preparing \n",
      "\u001b[1B26adb0e7: Preparing \n",
      "\u001b[1Ba6760893: Preparing \n",
      "\u001b[1B7e1a518f: Preparing \n",
      "\u001b[1Be62a5faf: Preparing \n",
      "\u001b[1B63322813: Preparing \n",
      "\u001b[1Ba6bc3dd8: Preparing \n",
      "\u001b[1B87d02174: Preparing \n",
      "\u001b[1Bfe36b2f8: Preparing \n",
      "\u001b[1Be6872e4a: Preparing \n",
      "\u001b[1B523aff8f: Preparing \n",
      "\u001b[1B6d302ca0: Preparing \n",
      "\u001b[1B392658c6: Preparing \n",
      "\u001b[1Bb4944e7b: Preparing \n",
      "\u001b[1Bc8a79190: Preparing \n",
      "\u001b[1Bdff644b3: Preparing \n",
      "\u001b[1Bff006560: Preparing \n",
      "\u001b[1B7e38f2d8: Preparing \n",
      "\u001b[1B2ea48d23: Preparing \n",
      "\u001b[1B62fe140e: Preparing \n",
      "\u001b[1B6d30da44: Preparing \n",
      "\u001b[1Bd96b0113: Preparing \n",
      "\u001b[1B15d3a62b: Preparing \n",
      "\u001b[1B39edd0b8: Preparing \n",
      "\u001b[1Bd2b930fc: Preparing \n",
      "\u001b[1Bec0db89a: Preparing \n",
      "\u001b[1B49baa658: Preparing \n",
      "\u001b[1B56d8b3f9: Layer already exists 4kB8A\u001b[2K\u001b[33A\u001b[2K\u001b[32A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[29A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[21A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[3A\u001b[2Klatest: digest: sha256:29e377baf8c9172d84f3e51e9d06a0a5df02b5f39ad2866ec1265fda6fb82dce size: 8501\n",
      "\n",
      "#done\n"
     ]
    }
   ],
   "source": [
    "#first build docker\n",
    "!sh build_and_push.sh absa-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d81d9b",
   "metadata": {},
   "source": [
    "## step1: upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66e8dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "#sess = sage.Session()\n",
    "sess = sage.LocalSession()\n",
    "\n",
    "WORK_DIRECTORY = \"../data/tasd/shulex\"\n",
    "\n",
    "# S3 prefix\n",
    "prefix = \"shulex-datalab\"\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3df7d09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-847380964353/shulex-datalab'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc96d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating r3jr6r1fyy-algo-1-yq2ya ... \n",
      "Creating r3jr6r1fyy-algo-1-yq2ya ... done\n",
      "Attaching to r3jr6r1fyy-algo-1-yq2ya\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m sed: can't read changehostname.c: No such file or directory\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m \u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kchangehostname.c: No such file or directory\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m \u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kno input files\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m compilation terminated.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m \u001b[01m\u001b[Kgcc:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[Kchangehostname.o: No such file or directory\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m <<<run train!!\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m ERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m env:  {'additional_framework_parameters': {}, 'channel_input_dirs': {'training': '/opt/ml/input/data/training'}, 'current_host': 'algo-1-yq2ya', 'framework_module': 'sagemaker_pytorch_container.training:main', 'hosts': ['algo-1-yq2ya'], 'hyperparameters': {'task': 'tasd', 'dataset': 'shulex', 'model_name_or_path': 't5-base', 'paradigm': 'extraction', 'gradient_accumulation_steps': 2, 'eval_batch_size': 16, 'train_batch_size': 2, 'learning_rate': 0.0003, 'num_train_epochs': 1, 'do_train': 'True', 'do_eval': 'False', 'do_direct_eval': 'False'}, 'input_config_dir': '/opt/ml/input/config', 'input_data_config': {'training': {'TrainingInputMode': 'File'}}, 'input_dir': '/opt/ml/input', 'is_master': True, 'job_name': 'absa-train-2022-05-13-05-12-48-492', 'log_level': 20, 'master_hostname': 'algo-1-yq2ya', 'model_dir': '/opt/ml/model', 'module_dir': '/opt/ml/code', 'module_name': 'None', 'network_interface_name': 'eth0', 'num_cpus': 4, 'num_gpus': 1, 'output_data_dir': '/opt/ml/output/data', 'output_dir': '/opt/ml/output', 'output_intermediate_dir': '/opt/ml/output/intermediate', 'resource_config': {'current_host': 'algo-1-yq2ya', 'hosts': ['algo-1-yq2ya']}, 'user_entry_point': None}\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m type:  <class 'dict'>\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m args task:  tasd\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m env task:  tasd\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m \n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m  ============================== NEW EXP: TASD on shulex ============================== \n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m \n",
      "Downloading: 100% 792k/792k [00:00<00:00, 1.98MB/s]\n",
      "Downloading: 100% 1.39M/1.39M [00:00<00:00, 3.73MB/s]\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m Here is an example (from dev set) under `extraction` paradigm:\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m Total examples = 613 for /opt/ml/input/data/training/dev.txt\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m /opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m   FutureWarning,\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m Input : Awesome light Omg. I crochet and wanted to do arigumi but itâ€™s small crochet and my eyes are older. This has helped so much. I can see now and I can use it to magnify my work. Pretty dang awesome light.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m Output: (helped so much, work_well)\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m \n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m ****** Conduct Training ******\n",
      "Downloading: 100% 1.20k/1.20k [00:00<00:00, 1.07MB/s]\n",
      "Downloading: 100% 892M/892M [00:12<00:00, 68.7MB/s] \n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m GPU available: True, used: True\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m /opt/conda/lib/python3.6/site-packages/torch/cuda/__init__.py:125: UserWarning: \n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m Tesla T4 with CUDA capability sm_75 is not compatible with the current PyTorch installation.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m The current PyTorch install supports CUDA capabilities sm_35 sm_52 sm_60 sm_61 sm_70 compute_70.\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m If you want to use the Tesla T4 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m \n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m   warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m \n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m   | Name  | Type                       | Params\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m -----------------------------------------------------\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m 0 | model | T5ForConditionalGeneration | 222 M \n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m Total examples = 613 for /opt/ml/input/data/training/dev.txt\n",
      "Validation sanity check: 0it [00:00, ?it/s][2022-05-13 05:13:17.342 a51887bfd184:11 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m [2022-05-13 05:13:17.400 a51887bfd184:11 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Total examples = 4903 for /opt/ml/input/data/training/train.txt\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m /opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m   FutureWarning,\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m train Input : Perfect! Super easy to assemble and provides great, comfortable, lighting\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m train Output: (easy to assemble, easy_assembly)\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m self.hparams.n_gpu:  0\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m t_total:  2451.0\n",
      "\u001b[36mr3jr6r1fyy-algo-1-yq2ya |\u001b[0m Total examples = 613 for /opt/ml/input/data/training/dev.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to delete: /tmp/tmpza6lhq2t/algo-1-yq2ya Please remove it manually.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e8607ae203b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1768\u001b[0m         \"\"\"\n\u001b[1;32m   1769\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1770\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, Environment, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         training_job.start(\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         self.model_artifacts = self.container.train(\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         )\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, environment, job_name)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# _stream_output() doesn't have the command line. We will handle the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m         \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mexit_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name\n",
    "image = \"847380964353.dkr.ecr.us-west-2.amazonaws.com/absa-train\"\n",
    "\n",
    "sess = sage.LocalSession()\n",
    "\n",
    "hyperparameters = {\n",
    "    \"task\" : \"tasd\", \n",
    "    #\"data_root\" : \"/opt/ml/input/data/training\",\n",
    "    \"dataset\" : \"shulex\", \n",
    "    \"model_name_or_path\" : \"t5-base\", \n",
    "    \"paradigm\": \"extraction\",\n",
    "    \"gradient_accumulation_steps\": \"2\",\n",
    "    \"eval_batch_size\" :\"16\",\n",
    "    \"train_batch_size\" :\"2\",\n",
    "    \"learning_rate\" :\"3e-4\",\n",
    "    \"num_train_epochs\":\"1\",\n",
    "    \"do_train\":True,\n",
    "    \"do_eval\":False,\n",
    "    \"do_direct_eval\":False\n",
    "    #\"out_dir\":\"/opt/ml/output\",\n",
    "}\n",
    "\n",
    "train = sage.estimator.Estimator(\n",
    "    image,\n",
    "    role,\n",
    "    sagemaker_session=sess,\n",
    "    instance_type='local_gpu',\n",
    "    instance_count=1,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "\n",
    "train.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0c059",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-13 05:20:50 Starting - Starting the training job...\n",
      "2022-05-13 05:21:16 Starting - Preparing the instances for trainingProfilerReport-1652419250: InProgress\n",
      "......."
     ]
    }
   ],
   "source": [
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name\n",
    "image = \"847380964353.dkr.ecr.us-west-2.amazonaws.com/absa\"\n",
    "\n",
    "sess = sage.Session()\n",
    "hyperparameters = {\n",
    "    \"task\" : \"tasd\", \n",
    "    #\"data_root\" : \"/opt/ml/input/data/training\",\n",
    "    \"dataset\" : \"shulex\", \n",
    "    \"model_name_or_path\" : \"t5-base\", \n",
    "    \"paradigm\": \"extraction\",\n",
    "    \"gradient_accumulation_steps\": \"2\",\n",
    "    \"eval_batch_size\" :\"16\",\n",
    "    \"train_batch_size\" :\"2\",\n",
    "    \"learning_rate\" :\"3e-4\",\n",
    "    \"num_train_epochs\":\"1\",\n",
    "    \"do_train\":True,\n",
    "    \"do_eval\":False,\n",
    "    \"do_direct_eval\":False\n",
    "    #\"out_dir\":\"/opt/ml/output\",\n",
    "}\n",
    "\n",
    "train = sage.estimator.Estimator(\n",
    "    image,\n",
    "    role,\n",
    "    instance_count = 1,\n",
    "    sagemaker_session=sess,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "\n",
    "train.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b49d7e",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "When we're done with the endpoint, we can just delete it and the backing instances will be released.  Run the following cell to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d71e9387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddle-ep--2022-04-13-05-54-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '842e4591-7ecf-42bb-950e-8d9cb273a478',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '842e4591-7ecf-42bb-950e-8d9cb273a478',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 13 Apr 2022 06:02:27 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(endpoint_name)\n",
    "sage.delete_endpoint(EndpointName=endpoint_name)\n",
    "sage.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sage.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b5eccd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paddle-ep--2022-04-13-05-54-10'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31033f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
