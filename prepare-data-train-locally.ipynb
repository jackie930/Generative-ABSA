{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e91507b",
   "metadata": {},
   "source": [
    "## data-prepare - shulex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a01f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72dc063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< path valid!\n",
      "training size:  (4903, 6)\n",
      "test size:  (613, 5)\n",
      "validate size:  (613, 5)\n",
      "<<<finish data preparing!\n"
     ]
    }
   ],
   "source": [
    "#preprocess data\n",
    "def convert_label(x,only_tag=False):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        if only_tag:\n",
    "            res.append(i['tag'])\n",
    "        else:\n",
    "            res.append((i['span'],i['tag']))\n",
    "    return res\n",
    "\n",
    "def write_txt(df,path):\n",
    "    #output txt file\n",
    "    df = df.reset_index()\n",
    "    with open(path,'a')as f:\n",
    "        for i in range(len(df)):\n",
    "            f.write(\"{} {} ####{}\".format(df.loc[i,'title'],df.loc[i,'content'],df.loc[i,'label_conv']))\n",
    "            f.write('\\n')\n",
    "def mkdir_rm(folder):\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder) \n",
    "    os.mkdir(folder)\n",
    "    print (\"<< path valid!\")\n",
    "    \n",
    "def flag_oversample(x,over_sample_list):\n",
    "    res = 0\n",
    "    for i in x:\n",
    "        if i['tag'] in over_sample_list:\n",
    "            res =1\n",
    "    return res\n",
    "        \n",
    "def preprocess_data(input_file,output_path,over_sample=True):\n",
    "    file  = './shulex/shulex_data.jsonl'\n",
    "    #load \n",
    "    jsonObj = pd.read_json(path_or_buf=input_file, lines=True)\n",
    "    #convert label to (sentence, tag) list\n",
    "    jsonObj['label_conv'] = jsonObj['label'].map(lambda x:convert_label(x))\n",
    "    jsonObj['content'] = jsonObj['content'].map(lambda x:x.replace('\\n',''))\n",
    "    jsonObj['label_tag'] = jsonObj['label'].map(lambda x:','.join(convert_label(x,only_tag=True)))\n",
    "    #map the tag list into single lines\n",
    "    df=jsonObj.drop('label_tag', axis=1).join(jsonObj['label_tag'].str.split(',', expand=True).stack().reset_index(level=1, drop=True).rename('tag'))\n",
    "\n",
    "    #write the tag list\n",
    "    a_list = df['tag'].unique()\n",
    "    \n",
    "    #get oversample tag list\n",
    "    tag1 = df.groupby('tag').count().reset_index()\n",
    "    df_tag_res = tag1[['tag','title']]\n",
    "    df_tag_res.columns = ['tag','frequency']\n",
    "    over_sample_list = df_tag_res[df_tag_res['frequency']<50]['tag'].unique()\n",
    "\n",
    "    with open('tag.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % tag for tag in a_list)\n",
    "\n",
    "    #remove & remake the output folder \n",
    "    mkdir_rm(output_path)\n",
    "    \n",
    "    #train/test/val split\n",
    "    train, validate, test = np.split(jsonObj.sample(frac=1), [int(.8*len(jsonObj)), int(.9*len(jsonObj))])\n",
    "    #flag over sample\n",
    "    train['flag'] = train['label'].map(lambda x:flag_oversample(x,over_sample_list))\n",
    "    \n",
    "    #write the train tag numbert distribution\n",
    "    df_train_tag=jsonObj.drop('label_tag', axis=1).join(jsonObj['label_tag'].str.split(',', expand=True).stack().reset_index(level=1, drop=True).rename('tag'))\n",
    "    tag1 = df_train_tag.groupby('tag').count().reset_index()\n",
    "    df_res = tag1[['tag','title']]\n",
    "    df_res.columns = ['tag','train_frequency']\n",
    "    df_res.to_csv(os.path.join(output_path,'train_tag_distribution.csv'))\n",
    "    \n",
    "    #over sample\n",
    "    if over_sample==True:\n",
    "        train_sample = train[train['flag']==1]\n",
    "        for i in range(50):\n",
    "            train = pd.concat([train,train_sample])\n",
    "    \n",
    "    print (\"training size: \",train.shape)\n",
    "    print (\"test size: \",test.shape)\n",
    "    print (\"validate size: \",validate.shape)\n",
    "    \n",
    "    # write train/test/dev\n",
    "    write_txt(train,os.path.join(output_path,'train.txt'))\n",
    "    write_txt(test,os.path.join(output_path,'test.txt'))\n",
    "    write_txt(validate,os.path.join(output_path,'dev.txt'))\n",
    "    print (\"<<<finish data preparing!\")\n",
    "    \n",
    "input_file = './shulex/shulex_data.jsonl'\n",
    "output_path = './data/tasd/shulex'\n",
    "preprocess_data(input_file,output_path,over_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3307474d",
   "metadata": {},
   "source": [
    "# model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713f897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r requirement.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r requirement.txt (line 2)) (0.1.91)\n",
      "Requirement already satisfied: pytorch_lightning==0.8.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r requirement.txt (line 3)) (0.8.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.0.0->-r requirement.txt (line 1)) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.0.0->-r requirement.txt (line 1)) (2020.11.13)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.0.0->-r requirement.txt (line 1)) (1.19.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.0.0->-r requirement.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.0.0->-r requirement.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.0.0->-r requirement.txt (line 1)) (0.9.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.0.0->-r requirement.txt (line 1)) (4.62.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.0.0->-r requirement.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.0.0->-r requirement.txt (line 1)) (0.0.53)\n",
      "Requirement already satisfied: tensorboard>=1.14 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (2.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (1.7.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (0.18.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (1.46.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (3.3.7)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (2.6.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (3.19.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirement.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirement.txt (line 1)) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirement.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.0.0->-r requirement.txt (line 1)) (1.26.8)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.3->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging->transformers==4.0.0->-r requirement.txt (line 1)) (2.4.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.0.0->-r requirement.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.0.0->-r requirement.txt (line 1)) (7.1.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.0.0->-r requirement.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (4.8.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirement.txt (line 3)) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "939a7a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: editdistance in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install editdistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e5a89",
   "metadata": {},
   "source": [
    "## use pretrained model from amazon-review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f47d343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t   scheduler.pt\t\t    tokenizer.json\n",
      "optimizer.pt\t   special_tokens_map.json  trainer_state.json\n",
      "pytorch_model.bin  spiece.model\t\t    training_args.bin\n",
      "rng_state.pth\t   tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "#use pretrain from amazon-review\n",
    "#!aws s3 cp s3://sagemaker-us-west-2-847380964353/train-amazon-review-t5-base-10epoch-stepeval/output/model.tar.gz ./\n",
    "#!ls ./pretrain/checkpoint-44500/\n",
    "#unzip model file\n",
    "#!tar -zxvf model.tar.gz -C ./pretrain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3167cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
    "#\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
    "\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "752482ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ============================== NEW EXP: TASD on shulex ============================== \n",
      "\n",
      "Here is an example (from dev set) under `extraction` paradigm:\n",
      "Total examples = 613 for data/tasd/shulex/dev.txt\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Input : Alabaster glass Nice piece but was to small for me to use.\n",
      "Output: (was to small for me to use, short)\n",
      "\n",
      "****** Conduct Training ******\n",
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Checkpoint directory ./outputs/tasd/shulex/extraction exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "Total examples = 613 for data/tasd/shulex/dev.txt\n",
      "Total examples = 4903 for data/tasd/shulex/train.txt                            \n",
      "Total examples = 613 for data/tasd/shulex/dev.txt\n",
      "Epoch 1:  89%|███████▉ | 2451/2758 [07:15<00:54,  5.63it/s, loss=0.217, v_num=2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2452/2758 [07:15<00:54,  5.63it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2455/2758 [07:15<00:53,  5.63it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2458/2758 [07:16<00:53,  5.64it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2461/2758 [07:16<00:52,  5.64it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2464/2758 [07:16<00:52,  5.65it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2467/2758 [07:16<00:51,  5.65it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2470/2758 [07:16<00:50,  5.66it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2473/2758 [07:16<00:50,  5.66it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2476/2758 [07:16<00:49,  5.67it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2479/2758 [07:17<00:49,  5.67it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2482/2758 [07:17<00:48,  5.68it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2485/2758 [07:17<00:48,  5.68it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2488/2758 [07:17<00:47,  5.69it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2491/2758 [07:17<00:46,  5.69it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2494/2758 [07:17<00:46,  5.70it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2497/2758 [07:17<00:45,  5.70it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2500/2758 [07:18<00:45,  5.71it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2503/2758 [07:18<00:44,  5.71it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2506/2758 [07:18<00:44,  5.72it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2509/2758 [07:18<00:43,  5.72it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2512/2758 [07:18<00:42,  5.73it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2515/2758 [07:18<00:42,  5.73it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2518/2758 [07:18<00:41,  5.74it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2521/2758 [07:18<00:41,  5.74it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▏| 2524/2758 [07:19<00:40,  5.75it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▏| 2527/2758 [07:19<00:40,  5.75it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2530/2758 [07:19<00:39,  5.76it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2533/2758 [07:19<00:39,  5.76it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2536/2758 [07:19<00:38,  5.77it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2539/2758 [07:19<00:37,  5.77it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2542/2758 [07:19<00:37,  5.78it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2545/2758 [07:20<00:36,  5.78it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2548/2758 [07:20<00:36,  5.79it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2551/2758 [07:20<00:35,  5.79it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2554/2758 [07:20<00:35,  5.80it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2557/2758 [07:20<00:34,  5.80it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2560/2758 [07:20<00:34,  5.81it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2563/2758 [07:20<00:33,  5.81it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2566/2758 [07:21<00:33,  5.82it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2569/2758 [07:21<00:32,  5.82it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2572/2758 [07:21<00:31,  5.83it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2575/2758 [07:21<00:31,  5.83it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2578/2758 [07:21<00:30,  5.84it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2581/2758 [07:21<00:30,  5.84it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2584/2758 [07:21<00:29,  5.85it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2587/2758 [07:22<00:29,  5.85it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2590/2758 [07:22<00:28,  5.86it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2593/2758 [07:22<00:28,  5.86it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2596/2758 [07:22<00:27,  5.87it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2599/2758 [07:22<00:27,  5.87it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2602/2758 [07:22<00:26,  5.88it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▌| 2605/2758 [07:22<00:26,  5.88it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2608/2758 [07:22<00:25,  5.89it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2611/2758 [07:23<00:24,  5.89it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2614/2758 [07:23<00:24,  5.90it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2617/2758 [07:23<00:23,  5.90it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2620/2758 [07:23<00:23,  5.91it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2623/2758 [07:23<00:22,  5.91it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2626/2758 [07:23<00:22,  5.92it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2629/2758 [07:23<00:21,  5.92it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2632/2758 [07:24<00:21,  5.93it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 2635/2758 [07:24<00:20,  5.93it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 2638/2758 [07:24<00:20,  5.94it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 2641/2758 [07:24<00:19,  5.94it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 2644/2758 [07:24<00:19,  5.95it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 2647/2758 [07:24<00:18,  5.95it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 2650/2758 [07:24<00:18,  5.96it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 2653/2758 [07:25<00:17,  5.96it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 2656/2758 [07:25<00:17,  5.97it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 2659/2758 [07:25<00:16,  5.97it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 2662/2758 [07:25<00:16,  5.98it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 2665/2758 [07:25<00:15,  5.98it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 2668/2758 [07:25<00:15,  5.99it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 2671/2758 [07:25<00:14,  5.99it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 2674/2758 [07:26<00:14,  6.00it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 2677/2758 [07:26<00:13,  6.00it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 2680/2758 [07:26<00:12,  6.01it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 2683/2758 [07:26<00:12,  6.01it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 2686/2758 [07:26<00:11,  6.01it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 2689/2758 [07:26<00:11,  6.02it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 2692/2758 [07:26<00:10,  6.02it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 2695/2758 [07:26<00:10,  6.03it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 2698/2758 [07:27<00:09,  6.03it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 2701/2758 [07:27<00:09,  6.04it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 2704/2758 [07:27<00:08,  6.04it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 2707/2758 [07:27<00:08,  6.05it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 2710/2758 [07:27<00:07,  6.05it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 2713/2758 [07:27<00:07,  6.06it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 2716/2758 [07:27<00:06,  6.06it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▊| 2719/2758 [07:28<00:06,  6.07it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 2722/2758 [07:28<00:05,  6.07it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 2725/2758 [07:28<00:05,  6.08it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 2728/2758 [07:28<00:04,  6.08it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 2731/2758 [07:28<00:04,  6.09it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 2734/2758 [07:28<00:03,  6.09it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 2737/2758 [07:28<00:03,  6.10it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 2740/2758 [07:29<00:02,  6.10it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 2743/2758 [07:29<00:02,  6.11it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 2746/2758 [07:29<00:01,  6.11it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 2749/2758 [07:29<00:01,  6.12it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 2752/2758 [07:29<00:00,  6.12it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 2755/2758 [07:29<00:00,  6.12it/s, loss=0.217, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|█| 2758/2758 [07:29<00:00,  6.13it/s, loss=0.217, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|█| 2758/2758 [07:35<00:00,  6.05it/s, loss=0.217, v_num=2, val_los\u001b[A\n",
      "Finish training and saving the model!\n",
      "CPU times: user 8.06 s, sys: 2.18 s, total: 10.2 s\n",
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python main.py --task tasd \\\n",
    "            --dataset shulex \\\n",
    "            --paradigm extraction \\\n",
    "            --n_gpu '0' \\\n",
    "            --model_name_or_path t5-base \\\n",
    "            --do_train \\\n",
    "            --train_batch_size 2 \\\n",
    "            --gradient_accumulation_steps 2 \\\n",
    "            --eval_batch_size 2 \\\n",
    "            --learning_rate 3e-4 \\\n",
    "            --num_train_epochs 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92e005ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ============================== NEW EXP: TASD on shulex ============================== \n",
      "\n",
      "Here is an example (from dev set) under `extraction` paradigm:\n",
      "Total examples = 613 for data/tasd/shulex/dev.txt\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Input : Alabaster glass Nice piece but was to small for me to use.\n",
      "Output: (was to small for me to use, short)\n",
      "\n",
      "****** Conduct Evaluating with the last state ******\n",
      "\n",
      "Load the trained model from outputs/tasd/shulex/extraction/cktepoch=1.ckpt...\n",
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "<<< read lines\n",
      "Total examples = 613 for data/tasd/shulex/test.txt\n",
      "<<< load test data\n",
      "Total examples = 613 for data/tasd/shulex/test.txt\n",
      "<<<< start evaluate\n",
      "100%|█████████████████████████████████████████████| 5/5 [01:17<00:00, 15.59s/it]\n",
      "\n",
      "Results of raw output\n",
      "<<<< res {'would_recommend': {'n_tp': 37, 'n_gold': 53, 'n_pred': 39, 'precision': 0.9487179487179487, 'recall': 0.6981132075471698, 'f1': 0.8043478260869565}, 'long_life': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, '': {'n_tp': 0, 'n_gold': 8, 'n_pred': 6, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'satisfied_memory_function': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'used_item': {'n_tp': 0, 'n_gold': 4, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'support_AI_connecting': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'switch_work_poorly': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'cannot_tighten_screws': {'n_tp': 2, 'n_gold': 6, 'n_pred': 10, 'precision': 0.2, 'recall': 0.3333333333333333, 'f1': 0.25}, 'will_repurchase': {'n_tp': 26, 'n_gold': 34, 'n_pred': 36, 'precision': 0.7222222222222222, 'recall': 0.7647058823529411, 'f1': 0.7428571428571428}, 'nice_looking': {'n_tp': 22, 'n_gold': 66, 'n_pred': 68, 'precision': 0.3235294117647059, 'recall': 0.3333333333333333, 'f1': 0.3283582089552239}, 'a': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'fragile_packaging': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'for_storing': {'n_tp': 0, 'n_gold': 0, 'n_pred': 2, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'damaged_screws': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'for_crafting': {'n_tp': 3, 'n_gold': 6, 'n_pred': 5, 'precision': 0.6, 'recall': 0.5, 'f1': 0.5454545454545454}, 'unstable_base': {'n_tp': 5, 'n_gold': 9, 'n_pred': 13, 'precision': 0.38461538461538464, 'recall': 0.5555555555555556, 'f1': 0.4545454545454546}, 'bad_installation': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'cannot_connect_to_app': {'n_tp': 1, 'n_gold': 4, 'n_pred': 2, 'precision': 0.5, 'recall': 0.25, 'f1': 0.3333333333333333}, 'return_and_refund': {'n_tp': 24, 'n_gold': 31, 'n_pred': 33, 'precision': 0.7272727272727273, 'recall': 0.7741935483870968, 'f1': 0.7500000000000001}, 'cozy_light': {'n_tp': 0, 'n_gold': 7, 'n_pred': 3, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'nic': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'not_worth_the_price': {'n_tp': 12, 'n_gold': 22, 'n_pred': 20, 'precision': 0.6, 'recall': 0.5454545454545454, 'f1': 0.5714285714285713}, 'fragile_base': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'no_power_adapter': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'for_eyelash_worker': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'for_office': {'n_tp': 6, 'n_gold': 20, 'n_pred': 19, 'precision': 0.3157894736842105, 'recall': 0.3, 'f1': 0.3076923076923077}, 'not_get_hot_easily': {'n_tp': 0, 'n_gold': 5, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'unadjustable_height': {'n_tp': 0, 'n_gold': 3, 'n_pred': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'space_saving': {'n_tp': 8, 'n_gold': 24, 'n_pred': 12, 'precision': 0.6666666666666666, 'recall': 0.3333333333333333, 'f1': 0.4444444444444444}, 'match_the_decoration': {'n_tp': 1, 'n_gold': 5, 'n_pred': 7, 'precision': 0.14285714285714285, 'recall': 0.2, 'f1': 0.16666666666666666}, 'not_as_described': {'n_tp': 3, 'n_gold': 6, 'n_pred': 6, 'precision': 0.5, 'recall': 0.5, 'f1': 0.5}, 'as_described': {'n_tp': 11, 'n_gold': 19, 'n_pred': 17, 'precision': 0.6470588235294118, 'recall': 0.5789473684210527, 'f1': 0.6111111111111113}, 'for_growing': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'handy_remote': {'n_tp': 10, 'n_gold': 18, 'n_pred': 16, 'precision': 0.625, 'recall': 0.5555555555555556, 'f1': 0.5882352941176471}, 'hard_to_understand': {'n_tp': 0, 'n_gold': 2, 'n_pred': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'as_expected': {'n_tp': 41, 'n_gold': 53, 'n_pred': 55, 'precision': 0.7454545454545455, 'recall': 0.7735849056603774, 'f1': 0.7592592592592593}, 'fire_hazard': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'easier_assembly': {'n_tp': 0, 'n_gold': 0, 'n_pred': 2, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'not_recommend': {'n_tp': 9, 'n_gold': 14, 'n_pred': 15, 'precision': 0.6, 'recall': 0.6428571428571429, 'f1': 0.6206896551724138}, 'got_too_hot': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'appropriate_height': {'n_tp': 0, 'n_gold': 6, 'n_pred': 2, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'not_connect_to_app': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'bad_design': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'fixed_switch_position': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'nice': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'work_well': {'n_tp': 66, 'n_gold': 93, 'n_pred': 109, 'precision': 0.6055045871559633, 'recall': 0.7096774193548387, 'f1': 0.6534653465346535}, 'easy_a': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'worst_choice': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'large_coverage': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'unfit_shade_size': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'dim_light': {'n_tp': 5, 'n_gold': 8, 'n_pred': 11, 'precision': 0.45454545454545453, 'recall': 0.625, 'f1': 0.5263157894736842}, 'no_remote_control': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'without_instructions': {'n_tp': 1, 'n_gold': 2, 'n_pred': 2, 'precision': 0.5, 'recall': 0.5, 'f1': 0.5}, 'adjustable_angl': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'bad_assembly': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'for_reading': {'n_tp': 32, 'n_gold': 48, 'n_pred': 35, 'precision': 0.9142857142857143, 'recall': 0.6666666666666666, 'f1': 0.7710843373493975}, 'stable_base': {'n_tp': 0, 'n_gold': 0, 'n_pred': 2, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'glass_cracked': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'dissatisfied_foot_switch': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'dissatisfied_obvious_cable': {'n_tp': 0, 'n_gold': 7, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'damaged_socket': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'narrow_lighting': {'n_tp': 0, 'n_gold': 2, 'n_pred': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'for_leisure': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'best_choice': {'n_tp': 6, 'n_gold': 13, 'n_pred': 9, 'precision': 0.6666666666666666, 'recall': 0.46153846153846156, 'f1': 0.5454545454545455}, 'handy_touch_control': {'n_tp': 1, 'n_gold': 6, 'n_pred': 11, 'precision': 0.09090909090909091, 'recall': 0.16666666666666666, 'f1': 0.11764705882352942}, 'tight_rope': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'as_ storage_shelf': {'n_tp': 0, 'n_gold': 3, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'easy_assembly': {'n_tp': 197, 'n_gold': 210, 'n_pred': 220, 'precision': 0.8954545454545455, 'recall': 0.9380952380952381, 'f1': 0.9162790697674419}, 'practical': {'n_tp': 2, 'n_gold': 9, 'n_pred': 10, 'precision': 0.2, 'recall': 0.2222222222222222, 'f1': 0.2105263157894737}, 'worse_than_expected': {'n_tp': 4, 'n_gold': 9, 'n_pred': 7, 'precision': 0.5714285714285714, 'recall': 0.4444444444444444, 'f1': 0.5}, 'satisfied_magnetic_remote': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'heavy': {'n_tp': 3, 'n_gold': 11, 'n_pred': 8, 'precision': 0.375, 'recall': 0.2727272727272727, 'f1': 0.3157894736842105}, 'cutting_too_sharp': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'brand_appeal': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'eyes_caring': {'n_tp': 0, 'n_gold': 5, 'n_pred': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'not_for_office': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'for_nursery': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'worth_the_price': {'n_tp': 97, 'n_gold': 111, 'n_pred': 122, 'precision': 0.7950819672131147, 'recall': 0.8738738738738738, 'f1': 0.832618025751073}, 'broken_screws': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'weight_caring': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'not_as_decoration': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'saggy_lampshade': {'n_tp': 0, 'n_gold': 3, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'eas': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'not_on_the_shelf': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'multiple_color': {'n_tp': 1, 'n_gold': 3, 'n_pred': 1, 'precision': 1.0, 'recall': 0.3333333333333333, 'f1': 0.5}, 'unadjustable_brightness': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'light_weight': {'n_tp': 2, 'n_gold': 10, 'n_pred': 4, 'precision': 0.5, 'recall': 0.2, 'f1': 0.28571428571428575}, 'return_and': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'beautiful_packaging': {'n_tp': 3, 'n_gold': 11, 'n_pred': 5, 'precision': 0.6, 'recall': 0.2727272727272727, 'f1': 0.37499999999999994}, 'short_life': {'n_tp': 7, 'n_gold': 10, 'n_pred': 13, 'precision': 0.5384615384615384, 'recall': 0.7, 'f1': 0.608695652173913}, 'short': {'n_tp': 3, 'n_gold': 6, 'n_pred': 5, 'precision': 0.6, 'recall': 0.5, 'f1': 0.5454545454545454}, 'better_than_expected': {'n_tp': 11, 'n_gold': 15, 'n_pred': 15, 'precision': 0.7333333333333333, 'recall': 0.7333333333333333, 'f1': 0.7333333333333333}, 'satisfied_warranty_policy': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'fit_size': {'n_tp': 0, 'n_gold': 2, 'n_pred': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'bad_delivery_service': {'n_tp': 0, 'n_gold': 5, 'n_pred': 9, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'easy_to_control': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'ordered_wrong_item': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'adjustable_angles': {'n_tp': 3, 'n_gold': 14, 'n_pred': 9, 'precision': 0.3333333333333333, 'recall': 0.21428571428571427, 'f1': 0.2608695652173913}}\n",
      "{'total precision': 0.6506849315068494, 'total recall': 0.6191806331471136, 'total f1': 0.6345419847328245}\n",
      "raw results:  [['better_than_expected'], ['return_and_refund', 'cannot_tighten_screws'], ['easy_assembly', 'work_well'], ['work_well'], ['for_office']]\n",
      "all_predictions_fixed:  [['better_than_expected'], ['return_and_refund', 'cannot_tighten_screws'], ['easy_assembly', 'work_well'], ['work_well'], ['for_office']]\n",
      "\n",
      "Results of fixed output\n",
      "<<<< res {'would_recommend': {'n_tp': 37, 'n_gold': 53, 'n_pred': 39, 'precision': 0.9487179487179487, 'recall': 0.6981132075471698, 'f1': 0.8043478260869565}, '': {'n_tp': 0, 'n_gold': 8, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'for_nursery': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'damaged_socket ': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'for_eyelash_worker': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'worth_the_price': {'n_tp': 97, 'n_gold': 111, 'n_pred': 122, 'precision': 0.7950819672131147, 'recall': 0.8738738738738738, 'f1': 0.832618025751073}, 'pole_not_screw': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'easy_a': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'for_office': {'n_tp': 6, 'n_gold': 20, 'n_pred': 19, 'precision': 0.3157894736842105, 'recall': 0.3, 'f1': 0.3076923076923077}, 'not_get_hot_easily': {'n_tp': 0, 'n_gold': 5, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'unadjustable_height': {'n_tp': 0, 'n_gold': 3, 'n_pred': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'satisfied_memory_function': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'space_saving': {'n_tp': 8, 'n_gold': 24, 'n_pred': 12, 'precision': 0.6666666666666666, 'recall': 0.3333333333333333, 'f1': 0.4444444444444444}, 'match_the_decoration': {'n_tp': 1, 'n_gold': 5, 'n_pred': 7, 'precision': 0.14285714285714285, 'recall': 0.2, 'f1': 0.16666666666666666}, 'large_coverage': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'used_item': {'n_tp': 0, 'n_gold': 4, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'unfit_shade_size': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'dim_light': {'n_tp': 5, 'n_gold': 8, 'n_pred': 11, 'precision': 0.45454545454545453, 'recall': 0.625, 'f1': 0.5263157894736842}, 'not_as_described': {'n_tp': 3, 'n_gold': 6, 'n_pred': 7, 'precision': 0.42857142857142855, 'recall': 0.5, 'f1': 0.4615384615384615}, 'support_AI_connecting': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'switch_work_poorly': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'no_power_adapter': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'as_described': {'n_tp': 11, 'n_gold': 19, 'n_pred': 17, 'precision': 0.6470588235294118, 'recall': 0.5789473684210527, 'f1': 0.6111111111111113}, 'no_remote_control': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'saggy_lampshade': {'n_tp': 0, 'n_gold': 3, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'without_instructions': {'n_tp': 1, 'n_gold': 2, 'n_pred': 2, 'precision': 0.5, 'recall': 0.5, 'f1': 0.5}, 'cannot_tighten_screws': {'n_tp': 2, 'n_gold': 6, 'n_pred': 10, 'precision': 0.2, 'recall': 0.3333333333333333, 'f1': 0.25}, 'for_reading': {'n_tp': 32, 'n_gold': 48, 'n_pred': 38, 'precision': 0.8421052631578947, 'recall': 0.6666666666666666, 'f1': 0.744186046511628}, 'will_repurchase': {'n_tp': 26, 'n_gold': 34, 'n_pred': 36, 'precision': 0.7222222222222222, 'recall': 0.7647058823529411, 'f1': 0.7428571428571428}, 'unadjustable_brightness': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'multiple_color': {'n_tp': 1, 'n_gold': 3, 'n_pred': 1, 'precision': 1.0, 'recall': 0.3333333333333333, 'f1': 0.5}, 'light_weight': {'n_tp': 2, 'n_gold': 10, 'n_pred': 4, 'precision': 0.5, 'recall': 0.2, 'f1': 0.28571428571428575}, 'dissatisfied_foot_switch': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'handy_remote': {'n_tp': 10, 'n_gold': 18, 'n_pred': 16, 'precision': 0.625, 'recall': 0.5555555555555556, 'f1': 0.5882352941176471}, 'nice_looking': {'n_tp': 22, 'n_gold': 66, 'n_pred': 68, 'precision': 0.3235294117647059, 'recall': 0.3333333333333333, 'f1': 0.3283582089552239}, 'damaged_socket': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'dissatisfied_obvious_cable': {'n_tp': 0, 'n_gold': 7, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'glass_cracked': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'narrow_lighting': {'n_tp': 0, 'n_gold': 2, 'n_pred': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'hard_to_understand': {'n_tp': 0, 'n_gold': 2, 'n_pred': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'for_leisure': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'best_choice': {'n_tp': 6, 'n_gold': 13, 'n_pred': 9, 'precision': 0.6666666666666666, 'recall': 0.46153846153846156, 'f1': 0.5454545454545455}, 'as_expected': {'n_tp': 41, 'n_gold': 53, 'n_pred': 55, 'precision': 0.7454545454545455, 'recall': 0.7735849056603774, 'f1': 0.7592592592592593}, 'for_crafting': {'n_tp': 3, 'n_gold': 6, 'n_pred': 5, 'precision': 0.6, 'recall': 0.5, 'f1': 0.5454545454545454}, 'beautiful_packaging': {'n_tp': 3, 'n_gold': 11, 'n_pred': 6, 'precision': 0.5, 'recall': 0.2727272727272727, 'f1': 0.3529411764705882}, 'unstable_base': {'n_tp': 5, 'n_gold': 9, 'n_pred': 16, 'precision': 0.3125, 'recall': 0.5555555555555556, 'f1': 0.39999999999999997}, 'fire_hazard': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'cannot_connect_to_app': {'n_tp': 2, 'n_gold': 4, 'n_pred': 3, 'precision': 0.6666666666666666, 'recall': 0.5, 'f1': 0.5714285714285715}, 'short': {'n_tp': 3, 'n_gold': 6, 'n_pred': 13, 'precision': 0.23076923076923078, 'recall': 0.5, 'f1': 0.3157894736842105}, 'better_than_expected': {'n_tp': 11, 'n_gold': 15, 'n_pred': 15, 'precision': 0.7333333333333333, 'recall': 0.7333333333333333, 'f1': 0.7333333333333333}, 'short_life': {'n_tp': 7, 'n_gold': 10, 'n_pred': 14, 'precision': 0.5, 'recall': 0.7, 'f1': 0.5833333333333334}, 'fit_size': {'n_tp': 0, 'n_gold': 2, 'n_pred': 2, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'handy_touch_control': {'n_tp': 1, 'n_gold': 6, 'n_pred': 11, 'precision': 0.09090909090909091, 'recall': 0.16666666666666666, 'f1': 0.11764705882352942}, 'satisfied_warranty_policy': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'bad_delivery_service': {'n_tp': 0, 'n_gold': 5, 'n_pred': 9, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'return_and_refund': {'n_tp': 25, 'n_gold': 31, 'n_pred': 34, 'precision': 0.7352941176470589, 'recall': 0.8064516129032258, 'f1': 0.7692307692307693}, 'cozy_light': {'n_tp': 0, 'n_gold': 7, 'n_pred': 3, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'not_recommend': {'n_tp': 9, 'n_gold': 14, 'n_pred': 15, 'precision': 0.6, 'recall': 0.6428571428571429, 'f1': 0.6206896551724138}, 'got_too_hot': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'easy_to_control': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'appropriate_height': {'n_tp': 0, 'n_gold': 6, 'n_pred': 2, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'as_ storage_shelf': {'n_tp': 0, 'n_gold': 3, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'easy_assembly': {'n_tp': 198, 'n_gold': 210, 'n_pred': 223, 'precision': 0.8878923766816144, 'recall': 0.9428571428571428, 'f1': 0.9145496535796767}, 'practical': {'n_tp': 2, 'n_gold': 9, 'n_pred': 10, 'precision': 0.2, 'recall': 0.2222222222222222, 'f1': 0.2105263157894737}, 'worse_than_expected': {'n_tp': 4, 'n_gold': 9, 'n_pred': 7, 'precision': 0.5714285714285714, 'recall': 0.4444444444444444, 'f1': 0.5}, 'bad_design': {'n_tp': 1, 'n_gold': 1, 'n_pred': 1, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'fixed_switch_position': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'not_worth_the_price': {'n_tp': 12, 'n_gold': 22, 'n_pred': 21, 'precision': 0.5714285714285714, 'recall': 0.5454545454545454, 'f1': 0.5581395348837208}, 'heavy': {'n_tp': 3, 'n_gold': 11, 'n_pred': 10, 'precision': 0.3, 'recall': 0.2727272727272727, 'f1': 0.28571428571428564}, 'satisfied_magnetic_remote': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'cutting_too_sharp': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'adjustable_angles': {'n_tp': 3, 'n_gold': 14, 'n_pred': 9, 'precision': 0.3333333333333333, 'recall': 0.21428571428571427, 'f1': 0.2608695652173913}, 'received_wrong_item': {'n_tp': 0, 'n_gold': 0, 'n_pred': 1, 'precision': 0.0, 'recall': 0, 'f1': 0}, 'work_well': {'n_tp': 66, 'n_gold': 93, 'n_pred': 109, 'precision': 0.6055045871559633, 'recall': 0.7096774193548387, 'f1': 0.6534653465346535}, 'brand_appeal': {'n_tp': 0, 'n_gold': 2, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, 'eyes_caring': {'n_tp': 0, 'n_gold': 5, 'n_pred': 1, 'precision': 0.0, 'recall': 0.0, 'f1': 0}}\n",
      "{'total precision': 0.6571709233791748, 'total recall': 0.6229050279329609, 'total f1': 0.6395793499043976}\n"
     ]
    }
   ],
   "source": [
    "!python main.py --task tasd \\\n",
    "            --dataset shulex \\\n",
    "            --ckpoint_path outputs/tasd/shulex/extraction/cktepoch=1.ckpt \\\n",
    "            --paradigm extraction \\\n",
    "            --n_gpu '0' \\\n",
    "            --do_direct_eval \\\n",
    "            --eval_batch_size 128 \\\n",
    "            --customer_jj False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c2a0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ============================== NEW EXP: TASD on shulex ============================== \n",
      "\n",
      "Here is an example (from dev set) under `extraction` paradigm:\n",
      "Total examples = 613 for data/tasd/shulex/dev.txt\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Input : Alabaster glass Nice piece but was to small for me to use.\n",
      "Output: (was to small for me to use, short)\n",
      "\n",
      "****** Conduct predicting with the last state ******\n",
      "\n",
      "Load the trained model from outputs/tasd/shulex/extraction/cktepoch=1.ckpt...\n",
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "tensor([    0,    41,  8894,   805,    72,     6,    56,   834,    60, 29446,\n",
      "           61,     1], device='cuda:0')\n",
      "sents: very high quality and sturdy Very high quality. We love this and intend to buy more. My 15 year old took mine!\n",
      "pred: (will buy more, will_repurchase)\n",
      "0.7281394004821777\n"
     ]
    }
   ],
   "source": [
    "!python main.py --task tasd \\\n",
    "            --dataset shulex \\\n",
    "            --ckpoint_path outputs/tasd/shulex/extraction/cktepoch=1.ckpt \\\n",
    "            --text \"very high quality and sturdy Very high quality. We love this and intend to buy more. My 15 year old took mine!\" \\\n",
    "            --paradigm extraction \\\n",
    "            --n_gpu 0 \\\n",
    "            --do_direct_predict \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57840578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ============================== NEW EXP: TASD on shulex ============================== \n",
      "\n",
      "Here is an example (from dev set) under `extraction` paradigm:\n",
      "Total examples = 613 for data/tasd/shulex/dev.txt\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Input : Alabaster glass Nice piece but was to small for me to use.\n",
      "Output: (was to small for me to use, short)\n",
      "\n",
      "****** Conduct predicting with the last state ******\n",
      "\n",
      "Load the trained model from outputs/tasd/shulex/extraction/cktepoch=1.ckpt...\n",
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "tensor([    0,    41,  4611,   427, 23325,     3, 22516,     6,   200,   834,\n",
      "         3995,   867,    61,     1], device='cuda:0')\n",
      "sents: THE FLOOR LAMP THE LAMP IS THE BEST EVER!!!\n",
      "pred: (THE BEST EVER, best_choice)\n",
      "0.7812728881835938\n",
      "CPU times: user 283 ms, sys: 111 ms, total: 394 ms\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python main.py --task tasd \\\n",
    "            --dataset shulex \\\n",
    "            --ckpoint_path outputs/tasd/shulex/extraction/cktepoch=1.ckpt \\\n",
    "            --text \"THE FLOOR LAMP THE LAMP IS THE BEST EVER!!!\" \\\n",
    "            --paradigm extraction \\\n",
    "            --n_gpu 0 \\\n",
    "            --do_direct_predict \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcbf47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
