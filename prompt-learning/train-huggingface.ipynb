{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71864ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2.86.2)\n",
      "Collecting sagemaker>=2.48.0\n",
      "  Downloading sagemaker-2.87.0.tar.gz (522 kB)\n",
      "     |████████████████████████████████| 522 kB 6.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "     |████████████████████████████████| 2.2 MB 33.9 MB/s            \n",
      "\u001b[?25hCollecting datasets[s3]==1.6.2\n",
      "  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
      "     |████████████████████████████████| 221 kB 47.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2020.11.13)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (4.8.3)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "     |████████████████████████████████| 3.3 MB 72.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2.26.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "     |████████████████████████████████| 895 kB 54.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.8)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.1) (1.19.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.70.12.2)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.3.4)\n",
      "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (6.0.1)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "     |████████████████████████████████| 69 kB 11.2 MB/s            \n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "     |████████████████████████████████| 211 kB 45.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.1.5)\n",
      "Collecting boto3==1.16.43\n",
      "  Downloading boto3-1.16.43-py2.py3-none-any.whl (130 kB)\n",
      "     |████████████████████████████████| 130 kB 45.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Collecting botocore==1.19.52\n",
      "  Downloading botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "     |████████████████████████████████| 7.2 MB 38.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.6.2) (0.10.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "     |████████████████████████████████| 73 kB 554 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (1.26.8)\n",
      "Requirement already satisfied: attrs==20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (20.3.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.8)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (3.19.1)\n",
      "Collecting sagemaker>=2.48.0\n",
      "  Using cached sagemaker-2.86.2-py2.py3-none-any.whl\n",
      "  Downloading sagemaker-2.86.1.tar.gz (521 kB)\n",
      "     |████████████████████████████████| 521 kB 79.6 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.86.0.tar.gz (521 kB)\n",
      "     |████████████████████████████████| 521 kB 75.8 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.85.0.tar.gz (521 kB)\n",
      "     |████████████████████████████████| 521 kB 74.8 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.84.0.tar.gz (520 kB)\n",
      "     |████████████████████████████████| 520 kB 71.4 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.83.0.tar.gz (520 kB)\n",
      "     |████████████████████████████████| 520 kB 71.5 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.82.2.tar.gz (520 kB)\n",
      "     |████████████████████████████████| 520 kB 66.6 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.82.1.tar.gz (520 kB)\n",
      "     |████████████████████████████████| 520 kB 73.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.82.0.tar.gz (520 kB)\n",
      "     |████████████████████████████████| 520 kB 58.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.81.1.tar.gz (519 kB)\n",
      "     |████████████████████████████████| 519 kB 44.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.81.0.tar.gz (519 kB)\n",
      "     |████████████████████████████████| 519 kB 44.4 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.80.0.tar.gz (517 kB)\n",
      "     |████████████████████████████████| 517 kB 42.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.79.0.tar.gz (516 kB)\n",
      "     |████████████████████████████████| 516 kB 41.5 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.78.0.tar.gz (513 kB)\n",
      "     |████████████████████████████████| 513 kB 70.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.77.1.tar.gz (513 kB)\n",
      "     |████████████████████████████████| 513 kB 64.0 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.77.0.tar.gz (513 kB)\n",
      "     |████████████████████████████████| 513 kB 66.6 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.76.0.tar.gz (512 kB)\n",
      "     |████████████████████████████████| 512 kB 43.7 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.75.1.tar.gz (511 kB)\n",
      "     |████████████████████████████████| 511 kB 64.6 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.75.0.tar.gz (511 kB)\n",
      "     |████████████████████████████████| 511 kB 69.0 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.74.0.tar.gz (481 kB)\n",
      "     |████████████████████████████████| 481 kB 72.8 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.73.0.tar.gz (481 kB)\n",
      "     |████████████████████████████████| 481 kB 43.1 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.72.3.tar.gz (475 kB)\n",
      "     |████████████████████████████████| 475 kB 34.4 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.72.2.tar.gz (473 kB)\n",
      "     |████████████████████████████████| 473 kB 59.0 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.72.1.tar.gz (473 kB)\n",
      "     |████████████████████████████████| 473 kB 43.6 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.72.0.tar.gz (477 kB)\n",
      "     |████████████████████████████████| 477 kB 72.8 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.71.0.tar.gz (477 kB)\n",
      "     |████████████████████████████████| 477 kB 73.3 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.70.0.tar.gz (466 kB)\n",
      "     |████████████████████████████████| 466 kB 70.1 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sagemaker-2.69.0.tar.gz (452 kB)\n",
      "     |████████████████████████████████| 452 kB 45.0 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging->transformers==4.6.1) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker>=2.48.0) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2021.5.30)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->datasets[s3]==1.6.2) (2021.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (0.3.0)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from s3fs->datasets[s3]==1.6.2) (1.3.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (0.7.1)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-2.2.0.tar.gz (59 kB)\n",
      "     |████████████████████████████████| 59 kB 11.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.1.2.tar.gz (58 kB)\n",
      "     |████████████████████████████████| 58 kB 832 kB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.1.1.tar.gz (57 kB)\n",
      "     |████████████████████████████████| 57 kB 8.6 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.1.0.tar.gz (54 kB)\n",
      "     |████████████████████████████████| 54 kB 4.9 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.0.1.tar.gz (54 kB)\n",
      "     |████████████████████████████████| 54 kB 689 kB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.0.0.tar.gz (52 kB)\n",
      "     |████████████████████████████████| 52 kB 2.7 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "     |████████████████████████████████| 52 kB 2.1 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.4.1.tar.gz (52 kB)\n",
      "     |████████████████████████████████| 52 kB 1.4 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.4.0.tar.gz (51 kB)\n",
      "     |████████████████████████████████| 51 kB 86 kB/s              \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.3.tar.gz (50 kB)\n",
      "     |████████████████████████████████| 50 kB 9.0 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
      "     |████████████████████████████████| 49 kB 8.9 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "     |████████████████████████████████| 48 kB 9.4 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "     |████████████████████████████████| 48 kB 7.3 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.3.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.8.1)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.12.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.6.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (0.13.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.2.0)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (5.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.2.0)\n",
      "Building wheels for collected packages: sagemaker, aiobotocore\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.69.0-py2.py3-none-any.whl size=625686 sha256=9e841de07ad74601d83ed6b865d39280c30a36de09bd9a9196045b7873bc3468\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/f4/2c/36/3ab4164cdc3413eb82504df7915934940baa5aa7fac923dc0f\n",
      "  Building wheel for aiobotocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiobotocore: filename=aiobotocore-1.2.2-py3-none-any.whl size=45730 sha256=d7694ffe4fab2b2d509f8ea84289b0443471b7a706894f22286d744b111e4a70\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/37/f3/76/dfc2d32494696a7e4710b2f57d9d15212226d19c42dc395865\n",
      "Successfully built sagemaker aiobotocore\n",
      "Installing collected packages: tqdm, filelock, botocore, xxhash, s3transfer, huggingface-hub, aiobotocore, tokenizers, sacremoses, datasets, boto3, transformers, sagemaker\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.3\n",
      "    Uninstalling tqdm-4.62.3:\n",
      "      Successfully uninstalled tqdm-4.62.3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.24.42\n",
      "    Uninstalling botocore-1.24.42:\n",
      "      Successfully uninstalled botocore-1.24.42\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.5.0\n",
      "    Uninstalling s3transfer-0.5.0:\n",
      "      Successfully uninstalled s3transfer-0.5.0\n",
      "  Attempting uninstall: aiobotocore\n",
      "    Found existing installation: aiobotocore 1.3.0\n",
      "    Uninstalling aiobotocore-1.3.0:\n",
      "      Successfully uninstalled aiobotocore-1.3.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.21.42\n",
      "    Uninstalling boto3-1.21.42:\n",
      "      Successfully uninstalled boto3-1.21.42\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.86.2\n",
      "    Uninstalling sagemaker-2.86.2:\n",
      "      Successfully uninstalled sagemaker-2.86.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.22.97 requires botocore==1.24.42, but you have botocore 1.19.52 which is incompatible.\n",
      "awscli 1.22.97 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.3.7 which is incompatible.\u001b[0m\n",
      "Successfully installed aiobotocore-1.2.2 boto3-1.16.43 botocore-1.19.52 datasets-1.6.2 filelock-3.4.1 huggingface-hub-0.0.8 s3transfer-0.3.7 sacremoses-0.0.49 sagemaker-2.69.0 tokenizers-0.10.3 tqdm-4.49.0 transformers-4.6.1 xxhash-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f094c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM role arn used for running training: arn:aws:iam::847380964353:role/spot-bot-SpotSageMakerExecutionRole-TP8BLT3Z5JJL\n",
      "S3 bucket used for storing artifacts: sagemaker-us-west-2-847380964353\n"
     ]
    }
   ],
   "source": [
    "import sagemaker.huggingface\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"IAM role arn used for running training: {role}\")\n",
    "print(f\"S3 bucket used for storing artifacts: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73aae0",
   "metadata": {},
   "source": [
    "## train-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc66b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test = pd.read_csv(os.path.join('./data/shulex/bert-pair', \"test_NLI_M.csv\"),header=None,sep=\"\\t\")\n",
    "train = pd.read_csv(os.path.join('./data/shulex/bert-pair', \"train_NLI_M.csv\"),header=None,sep=\"\\t\")\n",
    "\n",
    "test.columns=[\"idx\",\"label\",\"text\"]\n",
    "\n",
    "train.columns=[\"idx\",\"label\",\"text\"]\n",
    "\n",
    "\n",
    "train[[\"label\",\"text\"]].to_csv('./data/shulex/bert-pair/train.csv',index=False,encoding='utf-8')\n",
    "test[[\"label\",\"text\"]].to_csv('./data/shulex/bert-pair/test.csv',index=False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cfee40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5380</td>\n",
       "      <td>none</td>\n",
       "      <td>what do you think of the easy_assembly of it ?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5380</td>\n",
       "      <td>none</td>\n",
       "      <td>what do you think of the would_recommend of it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5380</td>\n",
       "      <td>none</td>\n",
       "      <td>what do you think of the will_repurchase of it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5380</td>\n",
       "      <td>none</td>\n",
       "      <td>what do you think of the light_weight of it ? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5380</td>\n",
       "      <td>none</td>\n",
       "      <td>what do you think of the worth_the_price of it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx label                                               text\n",
       "0  5380  none  what do you think of the easy_assembly of it ?...\n",
       "1  5380  none  what do you think of the would_recommend of it...\n",
       "2  5380  none  what do you think of the will_repurchase of it...\n",
       "3  5380  none  what do you think of the light_weight of it ? ...\n",
       "4  5380  none  what do you think of the worth_the_price of it..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01527c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "prefix='hp-datalab'\n",
    "\n",
    "bucket = sess.default_bucket() \n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train/train.csv\")\n",
    ").upload_file(\"./data/shulex/bert-pair/train.csv\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"test/test.csv\")\n",
    ").upload_file(\"./data/shulex/bert-pair/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58f022a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path = f's3://{sess.default_bucket()}/{prefix}/train/train.csv'\n",
    "test_input_path = f's3://{sess.default_bucket()}/{prefix}/test/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c464751",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'} # v4.6.1 is referring to the `transformers_version` you use in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f7c9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'per_device_train_batch_size':4,\n",
    "                 'per_device_eval_batch_size': 4,\n",
    "                 'model_name_or_path': 'roberta-large',\n",
    "                 'train_file':'/opt/ml/input/data/train/train.csv',\n",
    "                 'validation_file':'/opt/ml/input/data/test/test.csv',\n",
    "                 'test_file':'/opt/ml/input/data/test/test.csv',\n",
    "                 'do_train': True,\n",
    "                 'do_predict': True,\n",
    "                 'do_eval': True,\n",
    "                 'save_total_limit':3,\n",
    "                 'num_train_epochs': 3,\n",
    "                 'output_dir': '/opt/ml/model',\n",
    "                 'num_train_epochs': 1,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'seed': 7,\n",
    "                 'fp16': False,\n",
    "                 'eval_steps': 1000,\n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f94d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "      entry_point='run_glue.py', # script\n",
    "      source_dir='./examples/pytorch/text-classification', # relative path to example\n",
    "      git_config=git_config,\n",
    "      instance_type='ml.p3.8xlarge',\n",
    "      instance_count=1,\n",
    "      volume_size=500,\n",
    "      transformers_version='4.6',\n",
    "      pytorch_version='1.7',\n",
    "      py_version='py36',\n",
    "      role=role,\n",
    "      base_job_name='roberta-large-epoch3',\n",
    "      hyperparameters = hyperparameters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b37dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-25 08:53:42 Starting - Starting the training job...ProfilerReport-1650876815: InProgress\n",
      "...\n",
      "2022-04-25 08:54:28 Starting - Preparing the instances for training......\n",
      "2022-04-25 08:55:43 Downloading - Downloading input data...\n",
      "2022-04-25 08:56:06 Training - Downloading the training image..................\n",
      "2022-04-25 08:59:07 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-04-25 08:59:07,758 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-04-25 08:59:07,801 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-04-25 08:59:07,808 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-04-25 08:59:08,138 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting accelerate\n",
      "  Downloading accelerate-0.6.2-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 5)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from accelerate->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: accelerate\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.6.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-04-25 08:59:11,134 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_eval\": true,\n",
      "        \"do_predict\": true,\n",
      "        \"do_train\": true,\n",
      "        \"eval_steps\": 1000,\n",
      "        \"fp16\": false,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"model_name_or_path\": \"roberta-large\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"save_total_limit\": 3,\n",
      "        \"seed\": 7,\n",
      "        \"test_file\": \"/opt/ml/input/data/test/test.csv\",\n",
      "        \"train_file\": \"/opt/ml/input/data/train/train.csv\",\n",
      "        \"validation_file\": \"/opt/ml/input/data/test/test.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"roberta-large-epoch3-2022-04-25-08-53-34-970\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-847380964353/roberta-large-epoch3-2022-04-25-08-53-34-970/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.8xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.8xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"eval_steps\":1000,\"fp16\":false,\"learning_rate\":5e-05,\"model_name_or_path\":\"roberta-large\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"save_total_limit\":3,\"seed\":7,\"test_file\":\"/opt/ml/input/data/test/test.csv\",\"train_file\":\"/opt/ml/input/data/train/train.csv\",\"validation_file\":\"/opt/ml/input/data/test/test.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-847380964353/roberta-large-epoch3-2022-04-25-08-53-34-970/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"eval_steps\":1000,\"fp16\":false,\"learning_rate\":5e-05,\"model_name_or_path\":\"roberta-large\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"save_total_limit\":3,\"seed\":7,\"test_file\":\"/opt/ml/input/data/test/test.csv\",\"train_file\":\"/opt/ml/input/data/train/train.csv\",\"validation_file\":\"/opt/ml/input/data/test/test.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"roberta-large-epoch3-2022-04-25-08-53-34-970\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-847380964353/roberta-large-epoch3-2022-04-25-08-53-34-970/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_predict\",\"True\",\"--do_train\",\"True\",\"--eval_steps\",\"1000\",\"--fp16\",\"False\",\"--learning_rate\",\"5e-05\",\"--model_name_or_path\",\"roberta-large\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\",\"--save_total_limit\",\"3\",\"--seed\",\"7\",\"--test_file\",\"/opt/ml/input/data/test/test.csv\",\"--train_file\",\"/opt/ml/input/data/train/train.csv\",\"--validation_file\",\"/opt/ml/input/data/test/test.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_PREDICT=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=1000\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=false\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=roberta-large\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=3\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=7\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_FILE=/opt/ml/input/data/test/test.csv\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/train.csv\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/test/test.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_glue.py --do_eval True --do_predict True --do_train True --eval_steps 1000 --fp16 False --learning_rate 5e-05 --model_name_or_path roberta-large --num_train_epochs 1 --output_dir /opt/ml/model --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --save_total_limit 3 --seed 7 --test_file /opt/ml/input/data/test/test.csv --train_file /opt/ml/input/data/train/train.csv --validation_file /opt/ml/input/data/test/test.csv\u001b[0m\n",
      "\u001b[34m04/25/2022 08:59:15 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 4distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m04/25/2022 08:59:15 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr25_08-59-15_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=3, no_cuda=False, seed=7, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=4, mp_parameters=)\u001b[0m\n",
      "\u001b[34m04/25/2022 08:59:15 - INFO - __main__ -   load a local file for train: /opt/ml/input/data/train/train.csv\u001b[0m\n",
      "\u001b[34m04/25/2022 08:59:15 - INFO - __main__ -   load a local file for validation: /opt/ml/input/data/test/test.csv\u001b[0m\n",
      "\u001b[34m04/25/2022 08:59:15 - INFO - __main__ -   load a local file for test: /opt/ml/input/data/test/test.csv\u001b[0m\n",
      "\u001b[34m04/25/2022 08:59:16 - WARNING - datasets.builder -   Using custom data configuration default-e34ec8a4e61ccd5b\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-e34ec8a4e61ccd5b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e34ec8a4e61ccd5b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-25 08:59:21,692 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps71f4mk5\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-25 08:59:22,002 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-25 08:59:22,003 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2022-04-25 08:59:22,003 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-04-25 08:59:22,004 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:517] 2022-04-25 08:59:22,314 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:553] 2022-04-25 08:59:22,315 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-25 08:59:22,605 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_3uhasa7\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-25 08:59:23,325 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-25 08:59:23,325 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-25 08:59:23,626 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl3iu7gv5\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-25 08:59:24,302 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-25 08:59:24,302 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-25 08:59:24,608 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp550injzx\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-25 08:59:25,452 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-25 08:59:25,453 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-25 08:59:26,361 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-25 08:59:26,362 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-25 08:59:26,362 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-25 08:59:26,362 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-25 08:59:26,362 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1717] 2022-04-25 08:59:26,362 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2022-04-25 08:59:26,747 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu58se8oo\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2022-04-25 08:59:52,941 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2022-04-25 08:59:52,941 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1155] 2022-04-25 08:59:52,942 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1331] 2022-04-25 08:59:58,268 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1342] 2022-04-25 08:59:58,268 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m04/25/2022 09:01:15 - INFO - __main__ -   Sample 339563 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [0, 12196, 109, 47, 206, 9, 5, 542, 42009, 868, 1215, 23471, 1825, 9, 24, 17487, 85, 21, 10, 205, 2031, 4, 370, 64, 517, 5, 200, 1640, 23115, 43, 471, 25, 47, 2813, 4, 370, 67, 64, 304, 14, 233, 25, 892, 24272, 38, 4443, 4, 21277, 100, 33, 7, 8109, 14, 24, 16, 10, 828, 2016, 4, 36, 2409, 5, 223, 5, 24272, 2551, 101, 10, 828, 11216, 25, 47, 192, 15, 5, 2170, 4, 4839, 1708, 5, 1468, 8, 960, 1302, 182, 9869, 8, 24, 1326, 182, 182, 2579, 23, 5, 929, 4, 42326, 1779, 38, 1357, 5, 2233, 6, 38, 222, 45, 192, 5, 9223, 5462, 4, 36, 243, 21, 23, 182, 2576, 9, 5, 2233, 43, 407, 38, 554, 7, 6559, 15, 2], 'label': 0, 'text': 'what do you think of the unadjustable_brightness of it ? It was a good choice. You can move the second(small) head as you wish. You also can use that part as study lamp I guess. //I have to admit that it is a bit heavy. (And the under the lamp seemed like a bit dirty as you see on the picture. )But the material and everything seems very lovely and it looks very very nice at the room.//When I opened the box, I did not see the instructions sheet. (It was at very bottom of the box) So I started to assembly on my own. (Don’t start assembly from top of the lamp. Start from bottom, it makes it easier) .. Lastly, the bulbs are NOT included, so you may need to order it or buy it from somewhere. //All in all, it is a nice floor lamp and totally is worth it.'}.\u001b[0m\n",
      "\u001b[34m04/25/2022 09:01:15 - INFO - __main__ -   Sample 158176 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [0, 12196, 109, 47, 206, 9, 5, 1099, 1215, 23414, 9, 24, 17487, 26590, 36185, 26646, 205, 12, 14498, 5202, 11, 65, 2125, 12, 19968, 62, 12, 3896, 425, 24514, 36185, 252, 341, 5, 21084, 42437, 1001, 21466, 424, 684, 7, 313, 11, 5, 9805, 4, 38, 95, 1240, 5, 94, 291, 728, 8143, 24, 62, 3358, 15472, 1318, 797, 4, 6871, 45, 441, 7, 21927, 11, 70, 3745, 53, 38, 351, 75, 28, 1375, 24, 182, 203, 98, 24, 18, 30795, 868, 3358, 20, 1542, 376, 16212, 11, 457, 4, 85, 18, 300, 10, 4204, 5299, 15, 299, 9, 24, 8, 16, 2828, 639, 127, 16433, 98, 456, 6, 30795, 868, 6, 53, 444, 31, 1969, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'text': \"what do you think of the bad_design of it ? Pros:- Looks good- Arrived in one piece- Lights up- Fair priceCons:- They used the cheapest styrofoam known to man in the packaging. I just spent the last 20 minutes cleaning it up.- Zero quality control. Was not able to screw in all pieces but I won't be moving it very much so it's tolerable.- The base came cracked in half. It's got a metal plate on top of it and is sitting behind my couch so again, tolerable, but far from perfect.\"}.\u001b[0m\n",
      "\u001b[34m04/25/2022 09:01:15 - INFO - __main__ -   Sample 414002 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [0, 12196, 109, 47, 206, 9, 5, 40, 1215, 12597, 16707, 9, 24, 17487, 38, 657, 42, 24272, 13, 127, 2600, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'text': 'what do you think of the will_repurchase of it ? I love this lamp for my reading'}.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:516] 2022-04-25 09:01:20,168 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1156] 2022-04-25 09:01:20,214 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1157] 2022-04-25 09:01:20,215 >>   Num examples = 984800\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1158] 2022-04-25 09:01:20,215 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1159] 2022-04-25 09:01:20,215 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1160] 2022-04-25 09:01:20,215 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1161] 2022-04-25 09:01:20,215 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1162] 2022-04-25 09:01:20,215 >>   Total optimization steps = 61550\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.462 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.602 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.603 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.603 algo-1:32 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.605 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.605 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.850 algo-1:32 INFO hook.py:591] name:module.roberta.embeddings.word_embeddings.weight count_params:51471360\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.850 algo-1:32 INFO hook.py:591] name:module.roberta.embeddings.position_embeddings.weight count_params:526336\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.850 algo-1:32 INFO hook.py:591] name:module.roberta.embeddings.token_type_embeddings.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.850 algo-1:32 INFO hook.py:591] name:module.roberta.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.850 algo-1:32 INFO hook.py:591] name:module.roberta.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.850 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.851 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.851 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.851 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.851 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.851 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.851 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.851 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.851 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.851 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.852 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.853 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.854 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.855 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.856 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.857 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.858 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.859 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.860 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.861 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.862 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.863 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.864 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.865 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.866 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.867 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.868 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.869 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.870 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.871 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.872 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.872 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.872 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.872 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.872 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.872 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.872 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.872 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.872 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.873 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.873 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.873 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.873 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.873 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.873 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.873 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.873 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.874 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.874 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.874 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.874 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.874 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.874 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.874 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.874 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.874 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.875 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.875 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.875 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.875 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.875 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.875 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.875 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.875 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.875 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.875 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.876 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.876 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.876 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.876 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.876 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.876 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.876 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.876 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.876 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.877 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.877 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.877 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.877 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.877 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.877 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.877 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.877 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.878 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.879 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.879 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.879 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.879 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.879 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.879 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.879 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.879 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.879 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.880 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.880 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.880 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.880 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.880 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.880 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.880 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.880 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.880 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.881 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.881 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.881 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.881 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.881 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.881 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.881 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.881 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.881 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.881 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.882 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.882 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.882 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.882 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.882 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.882 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.882 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.882 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.882 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.883 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.884 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.885 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.885 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.885 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.885 algo-1:32 INFO hook.py:591] name:module.roberta.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.885 algo-1:32 INFO hook.py:591] name:module.classifier.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.885 algo-1:32 INFO hook.py:591] name:module.classifier.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.885 algo-1:32 INFO hook.py:591] name:module.classifier.out_proj.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.885 algo-1:32 INFO hook.py:591] name:module.classifier.out_proj.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.885 algo-1:32 INFO hook.py:593] Total Trainable Params: 355361794\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.886 algo-1:32 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-04-25 09:01:20.888 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34mNCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m{'loss': 0.0942, 'learning_rate': 4.959382615759545e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 09:09:06,466 >> Saving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 09:09:06,467 >> Configuration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 09:09:09,290 >> Model weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 09:09:09,291 >> tokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 09:09:09,292 >> Special tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.0811, 'learning_rate': 4.91876523151909e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 09:16:53,455 >> Saving model checkpoint to /opt/ml/model/checkpoint-1000\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 09:16:53,456 >> Configuration saved in /opt/ml/model/checkpoint-1000/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 09:16:56,252 >> Model weights saved in /opt/ml/model/checkpoint-1000/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 09:16:56,253 >> tokenizer config file saved in /opt/ml/model/checkpoint-1000/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 09:16:56,253 >> Special tokens file saved in /opt/ml/model/checkpoint-1000/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.0898, 'learning_rate': 4.8781478472786354e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 09:24:41,288 >> Saving model checkpoint to /opt/ml/model/checkpoint-1500\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 09:24:41,289 >> Configuration saved in /opt/ml/model/checkpoint-1500/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 09:24:44,075 >> Model weights saved in /opt/ml/model/checkpoint-1500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 09:24:44,076 >> tokenizer config file saved in /opt/ml/model/checkpoint-1500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 09:24:44,077 >> Special tokens file saved in /opt/ml/model/checkpoint-1500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.0857, 'learning_rate': 4.837530463038181e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 09:32:28,571 >> Saving model checkpoint to /opt/ml/model/checkpoint-2000\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 09:32:28,573 >> Configuration saved in /opt/ml/model/checkpoint-2000/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 09:32:31,362 >> Model weights saved in /opt/ml/model/checkpoint-2000/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 09:32:31,363 >> tokenizer config file saved in /opt/ml/model/checkpoint-2000/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 09:32:31,364 >> Special tokens file saved in /opt/ml/model/checkpoint-2000/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1961] 2022-04-25 09:32:36,997 >> Deleting older checkpoint [/opt/ml/model/checkpoint-500] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0969, 'learning_rate': 4.796913078797726e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 09:40:17,090 >> Saving model checkpoint to /opt/ml/model/checkpoint-2500\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 09:40:17,091 >> Configuration saved in /opt/ml/model/checkpoint-2500/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 09:40:19,869 >> Model weights saved in /opt/ml/model/checkpoint-2500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 09:40:19,870 >> tokenizer config file saved in /opt/ml/model/checkpoint-2500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 09:40:19,871 >> Special tokens file saved in /opt/ml/model/checkpoint-2500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1961] 2022-04-25 09:40:25,538 >> Deleting older checkpoint [/opt/ml/model/checkpoint-1000] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0783, 'learning_rate': 4.7562956945572706e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 09:48:04,161 >> Saving model checkpoint to /opt/ml/model/checkpoint-3000\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 09:48:04,162 >> Configuration saved in /opt/ml/model/checkpoint-3000/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 09:48:06,999 >> Model weights saved in /opt/ml/model/checkpoint-3000/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 09:48:07,001 >> tokenizer config file saved in /opt/ml/model/checkpoint-3000/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 09:48:07,001 >> Special tokens file saved in /opt/ml/model/checkpoint-3000/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1961] 2022-04-25 09:48:12,628 >> Deleting older checkpoint [/opt/ml/model/checkpoint-1500] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0958, 'learning_rate': 4.7156783103168155e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 09:55:51,433 >> Saving model checkpoint to /opt/ml/model/checkpoint-3500\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 09:55:51,434 >> Configuration saved in /opt/ml/model/checkpoint-3500/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 09:55:54,221 >> Model weights saved in /opt/ml/model/checkpoint-3500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 09:55:54,223 >> tokenizer config file saved in /opt/ml/model/checkpoint-3500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 09:55:54,224 >> Special tokens file saved in /opt/ml/model/checkpoint-3500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1961] 2022-04-25 09:55:59,892 >> Deleting older checkpoint [/opt/ml/model/checkpoint-2000] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0972, 'learning_rate': 4.675060926076361e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 10:03:39,653 >> Saving model checkpoint to /opt/ml/model/checkpoint-4000\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 10:03:39,654 >> Configuration saved in /opt/ml/model/checkpoint-4000/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 10:03:42,445 >> Model weights saved in /opt/ml/model/checkpoint-4000/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 10:03:42,446 >> tokenizer config file saved in /opt/ml/model/checkpoint-4000/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 10:03:42,447 >> Special tokens file saved in /opt/ml/model/checkpoint-4000/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1961] 2022-04-25 10:03:48,049 >> Deleting older checkpoint [/opt/ml/model/checkpoint-2500] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0919, 'learning_rate': 4.634443541835906e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 10:11:27,824 >> Saving model checkpoint to /opt/ml/model/checkpoint-4500\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 10:11:27,825 >> Configuration saved in /opt/ml/model/checkpoint-4500/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 10:11:30,608 >> Model weights saved in /opt/ml/model/checkpoint-4500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 10:11:30,609 >> tokenizer config file saved in /opt/ml/model/checkpoint-4500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 10:11:30,609 >> Special tokens file saved in /opt/ml/model/checkpoint-4500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1961] 2022-04-25 10:11:36,250 >> Deleting older checkpoint [/opt/ml/model/checkpoint-3000] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0944, 'learning_rate': 4.593826157595451e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 10:19:15,597 >> Saving model checkpoint to /opt/ml/model/checkpoint-5000\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 10:19:15,598 >> Configuration saved in /opt/ml/model/checkpoint-5000/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 10:19:18,462 >> Model weights saved in /opt/ml/model/checkpoint-5000/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 10:19:18,463 >> tokenizer config file saved in /opt/ml/model/checkpoint-5000/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 10:19:18,463 >> Special tokens file saved in /opt/ml/model/checkpoint-5000/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1961] 2022-04-25 10:19:24,168 >> Deleting older checkpoint [/opt/ml/model/checkpoint-3500] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0815, 'learning_rate': 4.553208773354996e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1885] 2022-04-25 10:27:02,192 >> Saving model checkpoint to /opt/ml/model/checkpoint-5500\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2022-04-25 10:27:02,194 >> Configuration saved in /opt/ml/model/checkpoint-5500/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2022-04-25 10:27:04,995 >> Model weights saved in /opt/ml/model/checkpoint-5500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1924] 2022-04-25 10:27:04,997 >> tokenizer config file saved in /opt/ml/model/checkpoint-5500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1930] 2022-04-25 10:27:04,998 >> Special tokens file saved in /opt/ml/model/checkpoint-5500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1961] 2022-04-25 10:27:10,824 >> Deleting older checkpoint [/opt/ml/model/checkpoint-4000] due to args.save_total_limit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'train':training_input_path,'test':test_input_path})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41bdd2",
   "metadata": {},
   "source": [
    "## test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244e928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df391c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62a64136",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b8af4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=\"s3://sagemaker-us-west-2-847380964353/xlm-roberta-base-epoch1-2021-11-08-08-17-03-658/output/model.tar.gz\",  # path to your trained sagemaker model\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.6\", # transformers version used\n",
    "   pytorch_version=\"1.7\", # pytorch version used\n",
    "   py_version=\"py36\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab7755",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.g4dn.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b145c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"The new Hugging Face SageMaker DLC makes it super easy to deploy models in production. I love it!\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
